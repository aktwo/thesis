In its most general form, the multi-armed bandit problem is a sequential decision-making problem where the decision-maker must explore new possibilities while trying to exploit existing knowledge to maximize the received payout. Using the notation in \cite{bubeck12}, there are are $K \geq{2}$ arms and sequences $X_{i, 1}, X_{i, 2}, ... $ for each arm $i = 1, ..., K$. At each time step, $t = 1, 2, ...$, the decision-maker picks $I_t \in \{1, ..., K\}$ and receives a payout $X_{I_t, t}$.

The common way of benchmarking the performance of a decision-making algorithm in this context is to take the distance between the expected payout of the algorithm and the optimal payout by play $n$. In the literature, optimality can be defined at two levels of granularity: at the play-by-play level or at the arm-level. Using the first notion of optimality yields the expected regret, which is defined in \ref{eq:ExpectedRegret}.

\begin{equation}
\label{eq:ExpectedRegret}
\mathbb{E}R_n = \mathbb{E}\left[ \max_{i=1, ..., K}{\sum_{t=1}^{n}{X_{i, t}}} - \sum_{t=1}^{n}{X_{I_t, t}}\right]
\end{equation}

A slightly weaker notion of regret comes from the second, more broad definition of optimality: picking the arm with the optimal expected payout. Using this notion of optimality yields the pseudo-regret, which is defined in \ref{eq:PseudoRegret}.

\begin{equation}
\label{eq:PseudoRegret}
\overline{R}_n = \max_{i=1, ..., K}\mathbb{E}\left[\sum_{t=1}^{n}{X_{i, t}} - \sum_{t=1}^{n}{X_{I_t, t}}\right]
\end{equation}

It is important to notice here that in \ref{eq:ExpectedRegret}, the algorithm is competing with the best possible random draw at every time-step, whereas in \ref{eq:PseudoRegret}, the algorithm is only competing with the arm with the highest expected value. This is the intuition for why $\overline{R}_n \leq{\mathbb{E}R_n}$.

Now that the general form of the multi-armed bandit problem has been introduced, there are three main sub-problems outlined by \cite{bubeck12} with further assumptions about the reward distributions. In the stochastic version, the sequences of plays $X_{i, 1}, X_{i, 2}, ... $ are I.I.D samples drawn from distributions $\nu_i \in [0, 1]$ for each arm $i = 1, ..., K$. In the adversarial version, at each time step $t$ an adversary selects a gain vector $g_t = (g_{1, t}, ..., g_{K, t}) \in [0, 1]^{K}$ such that $X_{i, t} = g_{i, t}$. In the Markovian version, the reward processes are neither i.i.d (as in the stochastic version) or chosen by an adversary (as in the adversarial version). Instead, each arm represents a Markov process with a state space $S$. Each time an arm $i$ is chosen in state $s \in{S}$, a reward is drawn from a probability distribution $\nu_{i, s}$ and the state of the Markov process for arm $i$ changes to $s' \in {S}$ based on a transition probability matrix $M_i$.

Armed with these three formulations of the multi-armed bandit problem, we can turn to the problem at hand: choosing conversation starters for a pair of TA users. Under the simplifying assumption that Princeton students react similarly to different conversation starters, at first glance it makes sense to use the stochastic formulation of the multi-armed bandit problem since the reward distribution could reasonably be assumed to be I.I.D. Additionally, the UCB1 algorithm in is elegant, computationally easy to implement, and has a logarithmic upper-bound on cumulative pseudo-regret when applied to a stochastic multi-armed bandit problem \citep{auer02}.

Although modeling the selection of conversation starters as a stochastic multi-armed bandit problem (and subsequently using the UCB1 algorithm) solves the first problem outlined in \ref{sec:WhyMultiArmedBandits} (i.e. picking the conversation starter that students would respond best to), it would actually exacerbate the second problem (i.e. seeing the same conversation starter repeatedly). Since the stochastic multi-armed bandit version of the problem assumes the reward distributions are I.I.D, it will simply hone in on the distribution with the highest expected value, and in the long term, will just pick the optimal arm at every play. In the context of TA, this would result in a single conversation starter always being displayed in the long term. This, in turn, would defeat the purpose of having a variety of conversation starters in the first place.

Since the arms chosen need to depend on the context (i.e. which users are chatting), a logical first choice would be to turn to a contextual bandit algorithm formulation. One example of such a personalized recommendation algorithm can be found in \cite{chu10}. \cite{chu10} observe the current user and a set of arms along with a feature vector for each user/arm pair. This vector summarizes information of both the user and arm, and can be thought of as the context. However, such a model would rapidly get out of hand for TA for a variety of reasons. First, there isn't just one user, but rather a pair of users, so one would have to maintain an extremely sparse database of all feature vectors for all arms, for all possible pairs of users. With a reasonable user-base of 1000 users and 200 conversation starters, this would result in 200,000,000 (e.g. $200 * 1000 * 1000$) feature vectors, many of which would be empty because the user-pair had not yet been observed. Second, the actual algorithm proposed in \cite{chu10} requires matrix multiplication and inversion to calculate the UCB value for each contextual bandit, which makes it computationally costly and infeasible for a web application handling multiple concurrent requests on a single server. Finally, having such a high level of context-dependency is unnecessary given the assumption that Princeton students' responses to a given conversation starter will be I.I.D.

This raises an important question: how do we take advantage of the simplifying assumption that Princeton students will respond similarly to any given conversation starter (i.e. the rewards are I.I.D) while still enforcing the invariant that a user doesn't see the same conversation starters repeatedly? This was the motivation to create a UCB algorithm somewhere between the context-free UCB1 and the context-dependent LinUCB. This new algorithm (which I have named UCB1-AKSB) is weakly context dependent, in that it applies the context-free UCB1 algorithm but dynamically filters the arms over which the algorithm to only include arms that neither user has already seen. This intuitive explanation is formalized in \ref{ch:Methods} below.
