In its most general form, the multi-armed bandit problem is a sequential decision-making problem where the decision-maker must explore new possibilities while trying to exploit existing knowledge in order to maximize the received reward. Using the notation in \cite{bubeck12}, there are are $K \geq{2}$ arms and sequences $X_{i, 1}, X_{i, 2}, \ldots $ for each arm $i = 1, \ldots, K$. At each time step $t = 1, 2, \ldots$ the decision-making algorithm picks $I_t \in \{1, \ldots, K\}$ and receives a reward $X_{I_t, t}$.

The common way of benchmarking the performance of a such an algorithm is to take the distance between the algorithm's expected reward and the optimal reward by play $n$. In the literature, this distance can be defined at two levels of granularity: at the play-by-play level or at the arm-level. Using the first notion of optimality yields the expected regret, which is defined in \autoref{eq:ExpectedRegret}.

\begin{equation}
\label{eq:ExpectedRegret}
\mathbb{E}R_n = \mathbb{E}\left[ \max_{i=1, \ldots, K}{\sum_{t=1}^{n}{X_{i, t}}} - \sum_{t=1}^{n}{X_{I_t, t}}\right]
\end{equation}

A slightly weaker notion of regret comes from the second definition of optimality: picking the arm with the optimal expected reward. Using this notion of optimality yields the pseudo-regret, which is defined in \autoref{eq:PseudoRegret}.

\begin{equation}
\label{eq:PseudoRegret}
\overline{R}_n = \max_{i=1, \ldots, K}\mathbb{E}\left[\sum_{t=1}^{n}{X_{i, t}} - \sum_{t=1}^{n}{X_{I_t, t}}\right]
\end{equation}

It is important to notice here that in \autoref{eq:ExpectedRegret}, the algorithm is competing with the best possible arm at every time-step, whereas in \autoref{eq:PseudoRegret}, the algorithm is only competing with the arm with the highest expected value. This is the intuition for why $\overline{R}_n \leq{\mathbb{E}R_n}$.

Building upon this general problem formulation, there are three main sub-problems outlined by \cite{bubeck12} with further assumptions about the reward distributions of the bandit arms. In the stochastic version, the sequences of plays $X_{i, 1}, X_{i, 2}, \ldots $ are I.I.D samples drawn from distributions $\nu_i \in [0, 1]$ for each arm $i = 1, \ldots, K$. In the adversarial version, at each time step $t$ an adversary selects a gain vector $g_t = (g_{1, t}, \ldots, g_{K, t}) \in [0, 1]^{K}$ such that $X_{i, t} = g_{i, t}$. In the Markovian version, the reward processes are neither i.i.d (as in the stochastic version) or chosen by an adversary (as in the adversarial version). Instead, each arm represents a Markov process with a state space $S$. Each time an arm $i$ is chosen in state $s \in{S}$, a reward is drawn from a probability distribution $\nu_{i, s}$ and the state of the Markov process for arm $i$ changes to $s' \in {S}$ based on a transition probability matrix $M_i$.

Armed with these three canonical variants of the multi-armed bandit problem, we are ready to tackle the problem at hand: choosing conversation starters for a pair of TA users. Assuming that Princeton students will react similarly to different conversation starters, it seems intuitive to use the stochastic formulation of the multi-armed bandit problem since the reward distributions for each arm could reasonably be assumed to be I.I.D. Additionally, the stochastic bandit problem has a well-documented solution: the UCB1 algorithm, which is elegant, computationally easy to implement, and has a uniformly logarithmic upper-bound on cumulative pseudo-regret \citep{auer02}.

Although modeling the problem in this way solves the first problem outlined in \autoref{sec:WhyMultiArmedBandits} (i.e. picking the conversation starter that students would respond best to), it would actually exacerbate the second problem (i.e. seeing the same conversation starter repeatedly). Since the stochastic multi-armed bandit version of the problem assumes the arm reward distributions are I.I.D, it will hone in on the `best' arm, and in the long term, will simply display the same conversation starter over and over again. This, in turn, would defeat the purpose of having a variety of conversation starters in the first place.

Since the arms chosen need to depend on which users are chatting, a logical first choice would be to turn to a contextual bandit algorithm. One heavily-cited example of such an algorithm (LinUCB) can be found in \cite{chu10}. \cite{chu10} observe the current user and a set of arms along with a feature vector for each user/arm pair. This vector summarizes information of both the user and arm, and can be thought of as the context. However, such a model would be infeasible for TA for a variety of reasons. First, since the context is defined by a pair of users, one would have to maintain an extremely sparse database of all feature vectors for all arms, for all possible pairs of users. With a reasonable user-base of 1000 users and 200 conversation starters, this would result in 200,000,000 (e.g. $200 * 1000 * 1000$) feature vectors, many of which would be empty or very sparse. Second, the actual algorithm proposed in \cite{chu10} requires matrix multiplication and inversion to calculate the UCB value for each contextual bandit, which makes it computationally costly and difficult to implement practically for a web application handling multiple concurrent requests on a single server. Finally, having such a high level of context-dependency is unnecessary given the assumption that Princeton students' responses to a given conversation starter will be I.I.D.

This raises an important question: how do we take advantage of the simplifying assumption that Princeton students will respond similarly to any given conversation starter (i.e. the rewards are I.I.D) while still enforcing the context-dependent invariant that a user doesn't see the same conversation starters repeatedly? This was the motivation to create a UCB algorithm somewhere between the context-free UCB1 and the context-dependent LinUCB. This new algorithm (which I have named UCB1-AKSB) is weakly context dependent, in that it applies the context-free UCB1 algorithm but dynamically filters the arms over which the algorithm to only include arms that neither user has already seen. This intuitive explanation is formalized in \autoref{ch:Methods} below.
